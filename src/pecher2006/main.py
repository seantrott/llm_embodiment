# -*- coding: utf-8 -*-
"""imagebind_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10rIMM7AN-UbItziAynvjhUDzSvLql9qL

# Setup
"""

"""
git init
git remote add origin https://github.com/facebookresearch/ImageBind.git
git pull origin main
pip install .
pip install soundfile
"""
import requests
from imagebind import data
import torch
from imagebind.models import imagebind_model
from imagebind.models.imagebind_model import ModalityType
from io import BytesIO

from PIL import Image
from IPython.display import display, Audio


device = "cuda:0" if torch.cuda.is_available() else "cpu"

# Instantiate model
model = imagebind_model.imagebind_huge(pretrained=True)
model.eval()
model.to(device)


"""
# Examples
"""


text_list = ["A dog.", "A car", "A bird"]
image_paths = [".assets/dog_image.jpg",
               ".assets/car_image.jpg", ".assets/bird_image.jpg"]
audio_paths = [".assets/dog_audio.wav",
               ".assets/car_audio.wav", ".assets/bird_audio.wav"]


# Load data
inputs = {
    ModalityType.TEXT: data.load_and_transform_text(text_list, device),
    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),
}

with torch.no_grad():
    embeddings = model(inputs)

print(
    "Text x Vision: \n",
    torch.softmax(embeddings[ModalityType.TEXT] @
                  embeddings[ModalityType.VISION].T, dim=-1),
)


"""
Nail & Eagle
"""

# Download the images

for filename in ["eagle_o", "eagle_c", "nail_h", "nail_v"]:
  url = f"https://camrobjones.com/static/etc/{filename}.jpg"
  print(url)
  response = requests.get(url)
  img = Image.open(BytesIO(response.content))
  img.save(f".assets/{filename}.jpg")

text_list = ["He hammered the nail into the wall",
             "He hammered the nail into the floor"]
image_paths = [".assets/nail_h.jpg", ".assets/nail_v.jpg"]


# Load data
inputs = {
    ModalityType.TEXT: data.load_and_transform_text(text_list, device),
    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),
}

with torch.no_grad():
    embeddings = model(inputs)

print(
    "Text x Vision: \n",
    torch.softmax(embeddings[ModalityType.TEXT] @
                  embeddings[ModalityType.VISION].T, dim=-1),
)


# Eagle shape

text_list=[
   "The ranger saw the eagle in the sky.",
   "The ranger saw the eagle in the tree."]
image_paths=[".assets/eagle_o.jpg", ".assets/eagle_c.jpg"]

# Show the images
print("Showing Images:")
for image_path in image_paths:
    img = Image.open(image_path)
    img.show()

# Load data
inputs = {
    ModalityType.TEXT: data.load_and_transform_text(text_list, device),
    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),
}

with torch.no_grad():
    embeddings = model(inputs)

print(
    "Text x Vision: \n",
    torch.softmax(embeddings[ModalityType.TEXT] @ embeddings[ModalityType.VISION].T, dim=-1),
)

"""We see the same pattern, but with a much smaller effect. Img 1 more similar to 1, 2 to 2.

# Pecher et al. (2006)

**Note**: I uploaded the images all locally to get this started quickly, but if there's a better way to access them through GitHub that'd be ideal. Also nots ure I got everything applied correctly.
"""

import pandas as pd
import seaborn as sns

df = pd.read_csv("data/items.csv")
df.head(2)

all_results = []
for index, item in df.iterrows():

  sentence_a = item['shape_a']
  sentence_b = item['shape_b']

  picture_a = item['picture shape a']
  picture_b = item['picture shape b']

  img1 = Image.open(picture_a)
  img1.save(f".assets/picture_a.jpg")
  img2 = Image.open(picture_b)
  img2.save(f".assets/picture_b.jpg")

  text_list = [sentence_a, sentence_b]
  image_paths = [".assets/picture_a.jpg", ".assets/picture_b.jpg"]

  # Load data
  inputs = {
      ModalityType.TEXT: data.load_and_transform_text(text_list, device),
      ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),
  }

  with torch.no_grad():
      embeddings = model(inputs)

  results = torch.softmax(embeddings[ModalityType.TEXT] @ embeddings[ModalityType.VISION].T, dim=-1)


  all_results.append({
      'match_a': results[0][0].item(),
      'mismatch_a': results[0][1].item(),
      'match_b': results[1][0].item(),
      'mismatch_b': results[1][1].item(),
      'object': item['object']
  })

df = pd.DataFrame(all_results)
df.shape

df.shape

melted_df = pd.melt(df)
melted_df['Sentence'] = melted_df['variable'].apply(lambda x: x.split('_')[-1])
melted_df.head(2)

melted_df['Match/Mismatch'] = melted_df['variable'].apply(lambda x: x.split('_')[0])
melted_df.head(2)

melted_df = melted_df.rename(columns={'value': 'Probability'})
# Drop the 'variable' column
melted_df = melted_df.drop(columns=['variable'])
# Reorder columns
melted_df = melted_df[['Match/Mismatch', 'Probability', 'Sentence']]
melted_df.head(2)

melted_df['Probability']

# sns.stripplot(data = melted_df, x = "Match/Mismatch", y = "Probability", alpha = .3, hue = "Item")
sns.pointplot(data = melted_df, x = "Match/Mismatch", y = "Probability", hue = "Sentence")

