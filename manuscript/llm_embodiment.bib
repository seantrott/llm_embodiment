@misc{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  number = {arXiv:2010.11929},
  eprint = {2010.11929},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-11},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Dosovitskiy et al_2021_An Image is Worth 16x16 Words.pdf;/Users/cameron/Zotero/storage/3H9G2WLM/2010.html}
}

@phdthesis{forbesPhysicalSocialCommonsense2021,
  type = {Thesis},
  title = {From {{Physical}} to {{Social Commonsense}}: {{Natural Language}} and the {{Natural World}}},
  shorttitle = {From {{Physical}} to {{Social Commonsense}}},
  author = {Forbes, Maxwell},
  year = {2021},
  urldate = {2022-02-09},
  abstract = {Along with the meteoric rise of computation-hungry models, NLP research has also produced new handcrafted datasets. These datasets allow us to study problems that are difficult by web scraping alone. We can use such data to evaluate and extend machine learning models into new areas. One area of natural interest is work that connects NLP to the outside world. This dissertation describes four projects that present such datasets and computational models. Each project attempts to situate NLP in a context broader than text alone. As a common thread throughout, we make use of commonsense knowledge, either explicitly or implicitly. The first half of the dissertation covers two projects, Verb Physics and Social Chemistry, which contain explicit representations of commonsense knowledge. Respectively, they capture physical commonsense (e.g., that my house is bigger than I am) and social commonsense (e.g., that it's rude for my roommate to run the blender at 5am). The second half studies language production and evaluation. In this half, commonsense implicitly informs the work. Neural Naturalist addresses language generation from image comparisons. Scarecrow focuses on evaluating text generated by large language models. In the conclusion, we urge the field to embrace communication\textemdash not merely natural language\textemdash and thereby extend the richness of groundings we consider.},
  copyright = {none},
  langid = {american},
  keywords = {p2},
  annotation = {Accepted: 2022-01-26T23:23:22Z},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Forbes (2021) From Physical to Social Commonsense.pdf;/Users/cameron/Zotero/storage/VJ9BRTP5/48229.html}
}

@article{garciaTouchWordsDynamic2016a,
  title = {A Touch with Words: {{Dynamic}} Synergies between Manual Actions and Language},
  shorttitle = {A Touch with Words},
  author = {Garc{\'i}a, Adolfo M. and Ib{\'a}{\~n}ez, Agust{\'i}n},
  year = {2016},
  month = sep,
  journal = {Neuroscience and Biobehavioral Reviews},
  volume = {68},
  pages = {59--95},
  issn = {1873-7528},
  doi = {10/f829ht},
  abstract = {Manual actions are a hallmark of humanness. Their underlying neural circuitry gives rise to species-specific skills and interacts with language processes. In particular, multiple studies show that hand-related expressions - verbal units evoking manual activity - variously affect concurrent manual actions, yielding apparently controversial results (interference, facilitation, or null effects) in varied time windows. Through a systematic review of 108 experiments, we show that such effects are driven by several factors, such as the level of verbal processing, action complexity, and the time-lag between linguistic and motor processes. We reconcile key empirical patterns by introducing the Hand-Action-Network Dynamic Language Embodiment (HANDLE) model, an integrative framework based on neural coupling dynamics and predictive-coding principles. To conclude, we assess HANDLE against the backdrop of other action-cognition theories, illustrate its potential applications to understand high-level deficits in motor disorders, and discuss key challenges for further development. In sum, our work aligns with the 'pragmatic turn', moving away from passive and static representationalist perspectives to a more dynamic, enactive, and embodied conceptualization of cognitive processes.},
  langid = {english},
  pmid = {27189784},
  keywords = {Cognition,Comprehension,Concept Formation,Enactive cognition,Hand-related language,Language,Language embodiment,Manual-action networks,Predictive coding},
  annotation = {ZSCC: 0000082}
}

@inproceedings{girdharImagebindOneEmbedding2023,
  title = {Imagebind: {{One}} Embedding Space to Bind Them All},
  shorttitle = {Imagebind},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Girdhar, Rohit and {El-Nouby}, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  year = {2023},
  pages = {15180--15190},
  urldate = {2023-09-28},
  keywords = {â›” No DOI found},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Girdhar et al_2023_Imagebind.pdf}
}

@misc{girdharImageBindOneEmbedding2023,
  title = {{{ImageBind}}: {{One Embedding Space To Bind Them All}}},
  shorttitle = {{{ImageBind}}},
  author = {Girdhar, Rohit and {El-Nouby}, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
  year = {2023},
  month = may,
  number = {arXiv:2305.05665},
  eprint = {2305.05665},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-10},
  abstract = {We present IMAGEBIND, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. IMAGEBIND can leverage recent large scale vision-language models, and extends their zeroshot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications `out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-theart on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that IMAGEBIND serves as a new way to evaluate vision models for visual and non-visual tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {/Users/cameron/Zotero/storage/ZRDFXKL5/Girdhar et al. - 2023 - ImageBind One Embedding Space To Bind Them All.pdf}
}

@article{guoFastExplicitNeural2021,
  title = {Fast and {{Explicit Neural View Synthesis}}},
  author = {Guo, Pengsheng and Bautista, Miguel Angel and Colburn, Alex and Yang, Liang and Ulbricht, Daniel and Susskind, Joshua M. and Shan, Qi},
  year = {2021},
  journal = {arXiv preprint arXiv:2107.05775},
  eprint = {2107.05775},
  archiveprefix = {arxiv},
  annotation = {ZSCC: 0000000},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Guo et al (2021) Fast and Explicit Neural View Synthesis.pdf;/Users/cameron/Zotero/storage/XSKI3FS8/2107.html}
}

@misc{IMAGEBINDOneEmbedding,
  title = {{{IMAGEBIND}}: {{One Embedding Space To Bind Them All}} - {{Meta Research}}},
  shorttitle = {{{IMAGEBIND}}},
  journal = {Meta Research},
  urldate = {2023-09-28},
  abstract = {We present IMAGEBIND, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together.},
  howpublished = {https://research.facebook.com/publications/imagebind-one-embedding-space-to-bind-them-all/},
  langid = {english},
  file = {/Users/cameron/Zotero/storage/NUHKP3IM/imagebind-one-embedding-space-to-bind-them-all.html}
}

@article{lucyAreDistributionalRepresentations2017,
  title = {Are Distributional Representations Ready for the Real World? {{Evaluating}} Word Vectors for Grounded Perceptual Meaning},
  shorttitle = {Are Distributional Representations Ready for the Real World?},
  author = {Lucy, Li and Gauthier, Jon},
  year = {2017},
  month = may,
  journal = {arXiv:1705.11168 [cs]},
  eprint = {1705.11168},
  primaryclass = {cs},
  urldate = {2022-01-30},
  abstract = {Distributional word representation methods exploit word co-occurrences to build compact vector encodings of words. While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning. In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants. We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors. Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.},
  archiveprefix = {arxiv},
  keywords = {â›” No DOI found,archived,Computer Science - Computation and Language},
  annotation = {ZSCC: 0000045},
  file = {/Users/cameron/Documents/Papers/Bibliographies/cogsci_2022_affordances_nlm/Lucy Gauthier (2017) Are distributional representations ready for the real world.pdf;/Users/cameron/Zotero/storage/N64XIQ4E/1705.html}
}

@misc{moskvichevConceptARCBenchmarkEvaluating2023,
  title = {The {{ConceptARC Benchmark}}: {{Evaluating Understanding}} and {{Generalization}} in the {{ARC Domain}}},
  shorttitle = {The {{ConceptARC Benchmark}}},
  author = {Moskvichev, Arseny and Odouard, Victor Vikram and Mitchell, Melanie},
  year = {2023},
  month = may,
  number = {arXiv:2305.07141},
  eprint = {2305.07141},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-12},
  abstract = {The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture. In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet [2019]. In particular, we describe ConceptARC, a new, publicly available benchmark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around "concept groups" -- sets of problems that focus on specific concepts and that are vary in complexity and level of abstraction. We report results on testing humans on this benchmark as well as three machine solvers: the top two programs from a 2021 ARC competition and OpenAI's GPT-4. Our results show that humans substantially outperform the machine solvers on this benchmark, showing abilities to abstract and generalize concepts that are not yet captured by AI systems. We believe that this benchmark will spur improvements in the development of AI systems for conceptual abstraction and in the effective evaluation of such systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Moskvichev et al_2023_The ConceptARC Benchmark.pdf;/Users/cameron/Zotero/storage/5M6SPYL2/2305.html}
}

@misc{radfordLearningTransferableVisual2021a,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-10},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/cameron/Zotero/storage/AYCHXAPP/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf}
}

@article{recchiaTeachingAutoregressiveLanguage2021,
  title = {Teaching {{Autoregressive Language Models Complex Tasks By Demonstration}}},
  author = {Recchia, Gabriel},
  year = {2021},
  month = sep,
  urldate = {2022-01-15},
  abstract = {This paper demonstrates that by fine-tuning an autoregressive language model (GPT-Neo) on appropriately structured step-by-step demonstrations, it is possible to teach it to execute a mathematical task that has previously proved difficult for Transformers - longhand modulo operations - with a relatively small number of examples. Specifically, we fine-tune GPT-Neo to solve the numbers\_\_div\_remainder task from the DeepMind Mathematics Dataset; Saxton et al. (arXiv:1904.01557) reported below 40\% accuracy on this task with 2 million training examples. We show that after fine-tuning on 200 appropriately structured demonstrations of solving long division problems and reporting the remainders, the smallest available GPT-Neo model achieves over 80\% accuracy. This is achieved by constructing an appropriate dataset for fine-tuning, with no changes to the learning algorithm. These results suggest that fine-tuning autoregressive language models on small sets of well-crafted demonstrations may be a useful paradigm for enabling individuals without training in machine learning to coax such models to perform some kinds of complex multi-step tasks.},
  langid = {english},
  keywords = {â›” No DOI found},
  annotation = {ZSCC: 0000000},
  file = {/Users/cameron/Zotero/storage/QSBXNYEU/Recchia - 2021 - Teaching Autoregressive Language Models Complex Ta.pdf;/Users/cameron/Zotero/storage/W4K63KMS/2109.html}
}

@misc{schickToolformerLanguageModels2023,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04761},
  eprint = {2302.04761},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.04761},
  urldate = {2023-10-11},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\textbackslash\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Schick et al_2023_Toolformer.pdf;/Users/cameron/Zotero/storage/FXM88HKY/2302.html}
}

@article{schuhmannLaion5bOpenLargescale2022,
  title = {Laion-5b: {{An}} Open Large-Scale Dataset for Training next Generation Image-Text Models},
  shorttitle = {Laion-5b},
  author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell},
  year = {2022},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {25278--25294},
  urldate = {2023-10-11},
  keywords = {â›” No DOI found},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Schuhmann et al_2022_Laion-5b.pdf}
}

@article{suttonBitterLesson2019,
  title = {The Bitter Lesson},
  author = {Sutton, Richard},
  year = {2019},
  journal = {Incomplete Ideas (blog)},
  volume = {13},
  number = {1},
  urldate = {2023-09-28},
  keywords = {â›” No DOI found},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Sutton_2019_The bitter lesson.pdf}
}

@misc{touvronTrainingDataefficientImage2021,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  year = {2021},
  month = jan,
  number = {arXiv:2012.12877},
  eprint = {2012.12877},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.12877},
  urldate = {2023-10-11},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Touvron et al_2021_Training data-efficient image transformers & distillation through attention.pdf;/Users/cameron/Zotero/storage/59VDIU7S/2012.html}
}

@misc{wangEnableLanguageModels2023,
  title = {Enable {{Language Models}} to {{Implicitly Learn Self-Improvement From Data}}},
  author = {Wang, Ziqi and Hou, Le and Lu, Tianjian and Wu, Yuexin and Li, Yunxuan and Yu, Hongkun and Ji, Heng},
  year = {2023},
  month = oct,
  number = {arXiv:2310.00898},
  eprint = {2310.00898},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.00898},
  urldate = {2023-10-11},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). To this end, we propose an ImPlicit Self-ImprovemenT (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models without extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Wang et al_2023_Enable Language Models to Implicitly Learn Self-Improvement From Data.pdf;/Users/cameron/Zotero/storage/XRDYVXSV/2310.html}
}

@article{wangLanguageMediatedObjectCentricRepresentation2020,
  title = {Language-{{Mediated}}, {{Object-Centric Representation Learning}}},
  author = {Wang, Ruocheng and Mao, Jiayuan and Gershman, Samuel J. and Wu, Jiajun},
  year = {2020},
  journal = {arXiv preprint arXiv:2012.15814},
  eprint = {2012.15814},
  archiveprefix = {arxiv},
  annotation = {ZSCC: 0000003},
  file = {/Users/cameron/Documents/Papers/Multimodal Computational Models of Language Comprehension/Wang et al (2020) Language-Mediated, Object-Centric Representation Learning.pdf;/Users/cameron/Zotero/storage/AAMANQRN/2012.html}
}

@article{zwaanRevisitingMentalSimulation2012,
  title = {Revisiting {{Mental Simulation}} in {{Language Comprehension}}: {{Six Replication Attempts}}},
  shorttitle = {Revisiting {{Mental Simulation}} in {{Language Comprehension}}},
  author = {Zwaan, Rolf A. and Pecher, Diane},
  year = {2012},
  month = dec,
  journal = {PLOS ONE},
  volume = {7},
  number = {12},
  pages = {e51382},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0051382},
  urldate = {2023-10-13},
  abstract = {The notion of language comprehension as mental simulation has become popular in cognitive science. We revisit some of the original empirical evidence for this. Specifically, we attempted to replicate the findings from earlier studies that examined the mental simulation of object orientation, shape, and color, respectively, in sentence-picture verification. For each of these sets of findings, we conducted two web-based replication attempts using Amazon's Mechanical Turk. Our results are mixed. Participants responded faster to pictures that matched the orientation or shape implied by the sentence, replicating the original findings. The effect was larger and stronger for shape than orientation. Participants also responded faster to pictures that matched the color implied by the sentence, whereas the original studies obtained mismatch advantages. We argue that these results support mental simulation theory, show the importance of replication studies, and show the viability of web-based data collection.},
  langid = {english},
  keywords = {Bayesian method,Cognition,Eagles,Educational attainment,Experimental psychology,Grammar,Language,Replication studies},
  file = {/Users/cameron/Documents/Papers/Zwaan_Pecher_2012_Revisiting Mental Simulation in Language Comprehension.pdf}
}
